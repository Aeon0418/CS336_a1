{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7be4c1",
   "metadata": {},
   "source": [
    "实现tranformer模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformer_block(\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int,\n",
    "    max_seq_len: int,\n",
    "    theta: float,\n",
    "    weights: dict[str, Tensor],\n",
    "    in_features: Float[Tensor, \" batch sequence_length d_model\"],\n",
    ") -> Float[Tensor, \" batch sequence_length d_model\"]:\n",
    "    \"\"\"\n",
    "    给定预归一化Transformer块的权重和输入特征，\n",
    "    返回在输入特征上运行Transformer块的输出。\n",
    "\n",
    "    此函数应使用RoPE。\n",
    "    根据你的实现，你可能只需要将相关参数传递给\n",
    "    TransformerBlock构造函数，或者你可能需要初始化自己的RoPE\n",
    "    类并传递它。\n",
    "\n",
    "    参数:\n",
    "        d_model (int): Transformer块输入的维度。\n",
    "        num_heads (int): 多头注意力中使用的头数。`d_model`必须\n",
    "            能被`num_heads`整除。\n",
    "        d_ff (int): 前馈内层的维度。\n",
    "        max_seq_len (int): 如果你的实现预缓存的最大序列长度。\n",
    "        theta (float): RoPE参数。\n",
    "        weights (dict[str, Tensor]):\n",
    "            我们参考实现的状态字典。\n",
    "            此字典的键包括:\n",
    "            - `attn.q_proj.weight`\n",
    "                所有`num_heads`个注意力头的查询投影。\n",
    "                形状为(d_model, d_model)。\n",
    "                行按形状为(num_heads, d_k)的矩阵排序，\n",
    "                所以`attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`。\n",
    "            - `attn.k_proj.weight`\n",
    "                所有`num_heads`个注意力头的键投影。\n",
    "                形状为(d_model, d_model)。\n",
    "                行按形状为(num_heads, d_k)的矩阵排序，\n",
    "                所以`attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`。\n",
    "            - `attn.v_proj.weight`\n",
    "                所有`num_heads`个注意力头的值投影。\n",
    "                形状为(d_model, d_model)。\n",
    "                行按形状为(num_heads, d_v)的矩阵排序，\n",
    "                所以`attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`。\n",
    "            - `attn.output_proj.weight`\n",
    "                多头自注意力输出投影的权重\n",
    "                形状为(d_model, d_model)。\n",
    "            - `ln1.weight`\n",
    "                变换器块中应用的第一个RMSNorm的仿射变换权重。\n",
    "                形状为(d_model,)。\n",
    "            - `ffn.w1.weight`\n",
    "                FFN中第一个线性变换的权重。\n",
    "                形状为(d_model, d_ff)。\n",
    "            - `ffn.w2.weight`\n",
    "                FFN中第二个线性变换的权重。\n",
    "                形状为(d_ff, d_model)。\n",
    "            - `ffn.w3.weight`\n",
    "                FFN中第三个线性变换的权重。\n",
    "                形状为(d_model, d_ff)。\n",
    "            - `ln2.weight`\n",
    "                变换器块中应用的第二个RMSNorm的仿射变换权重。\n",
    "                形状为(d_model,)。\n",
    "        in_features (Float[Tensor, \"batch sequence_length d_model\"]):\n",
    "            运行实现的张量。\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor, \"batch sequence_length d_model\"] 在使用RoPE时\n",
    "        在输入特征上运行Transformer块的输出张量。\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e07f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block (预归一化版本)\n",
    "    \n",
    "    这是现代 Transformer 架构的基本构建块，采用预归一化设计，\n",
    "    比原始 Transformer 的后归一化更稳定、更容易训练。\n",
    "    \n",
    "    架构组成:\n",
    "    1. Pre-RMSNorm + Multi-Head Self-Attention + Residual Connection\n",
    "    2. Pre-RMSNorm + Feed-Forward Network (SwiGLU) + Residual Connection\n",
    "    \n",
    "    相比后归一化的优势:\n",
    "    - 梯度流更稳定\n",
    "    - 训练收敛更快\n",
    "    - 支持更深的网络\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 num_heads: int, \n",
    "                 d_ff: int, \n",
    "                 max_seq_len: int, \n",
    "                 theta: float | None = None):\n",
    "        \"\"\"\n",
    "        初始化 Transformer Block\n",
    "        \n",
    "        参数:\n",
    "            d_model (int): 模型的特征维度 (如 512, 768, 1024)\n",
    "                          必须能被 num_heads 整除\n",
    "            num_heads (int): 多头注意力的头数 (如 8, 12, 16)\n",
    "            d_ff (int): 前馈网络的隐藏层维度\n",
    "                       通常是 d_model 的 4 倍 (如 d_model=512 -> d_ff=2048)\n",
    "            max_seq_len (int): 支持的最大序列长度\n",
    "                              用于 RoPE 位置编码的预计算\n",
    "            theta (float | None): RoPE 的基础角度参数\n",
    "                                 如果为 None，则不使用位置编码\n",
    "                                 如果不为 None (如 10000.0)，则启用 RoPE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 保存关键参数用于调试和后续使用\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.theta = theta\n",
    "        \n",
    "        # 条件性初始化注意力模块\n",
    "        if theta is not None:\n",
    "            # 创建 RoPE 位置编码器\n",
    "            # d_k = d_model // num_heads：每个头的维度\n",
    "            pos_encode = RotaryPositionalEmbedding(\n",
    "                theta=theta, \n",
    "                d_k=d_model // num_heads,  # 注意：RoPE 应用于每个头\n",
    "                max_seq_len=max_seq_len\n",
    "            )\n",
    "            \n",
    "            # 带位置编码的多头自注意力\n",
    "            self.attn = Multihead_self_attention(\n",
    "                d_model=d_model, \n",
    "                num_heads=num_heads, \n",
    "                pos_encode=pos_encode, \n",
    "                theta=theta\n",
    "            )\n",
    "        else:\n",
    "            # 不带位置编码的多头自注意力\n",
    "            self.attn = Multihead_self_attention(\n",
    "                d_model=d_model, \n",
    "                num_heads=num_heads\n",
    "            )\n",
    "        \n",
    "        # Layer Normalization 层\n",
    "        # 使用 RMSNorm 替代传统 LayerNorm，计算更高效\n",
    "        self.rmsn_1 = RMSNorm(d_model=d_model, eps=1e-5)  # 注意力前的归一化\n",
    "        self.rmsn_2 = RMSNorm(d_model=d_model, eps=1e-5)  # FFN 前的归一化\n",
    "        \n",
    "        # Feed-Forward Network (FFN)\n",
    "        # 使用 SwiGLU 替代传统的 ReLU FFN，表达能力更强\n",
    "        # SwiGLU: SwiGLU(x) = (SiLU(xW1) ⊙ xW3)W2\n",
    "        self.pw_ffn = SwiGLU(d_model=d_model, d_ff=d_ff)\n",
    "        \n",
    "        # 备选：传统的 SiLU FFN (已注释)\n",
    "        # self.pw_ffn = SiLU(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transformer Block 前向传播 (预归一化版本)\n",
    "        \n",
    "        计算流程:\n",
    "        1. x -> RMSNorm -> Multi-Head Attention -> + x (第一个残差连接)\n",
    "        2. 结果 -> RMSNorm -> FFN -> + 结果 (第二个残差连接)\n",
    "        \n",
    "        参数:\n",
    "            x (torch.Tensor): 输入张量\n",
    "                             形状: (batch_size, sequence_length, d_model)\n",
    "        \n",
    "        返回:\n",
    "            torch.Tensor: 输出张量，形状与输入相同\n",
    "                         (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # === 第一个子层：Multi-Head Self-Attention ===\n",
    "        \n",
    "        # 1. 预归一化：在注意力计算前先归一化\n",
    "        # 这是与原始 Transformer 的关键差异 (原始是后归一化)\n",
    "        normalized_x = self.rmsn_1(x)\n",
    "        \n",
    "        # 2. 多头自注意力计算\n",
    "        # 输入和输出形状都是 (batch_size, sequence_length, d_model)\n",
    "        attn_output = self.attn(normalized_x)\n",
    "        \n",
    "        # 3. 第一个残差连接\n",
    "        # 将注意力输出与原始输入相加，保持梯度流\n",
    "        # 这使得网络可以学习恒等映射，有助于训练深层网络\n",
    "        out1 = x + attn_output\n",
    "        \n",
    "        # === 第二个子层：Feed-Forward Network ===\n",
    "        \n",
    "        # 4. 预归一化：在 FFN 计算前归一化\n",
    "        normalized_out1 = self.rmsn_2(out1)\n",
    "        \n",
    "        # 5. 前馈网络计算\n",
    "        # SwiGLU: 先上投影到 d_ff 维度，应用门控激活，再下投影回 d_model\n",
    "        # 形状变化: (batch, seq, d_model) -> (batch, seq, d_ff) -> (batch, seq, d_model)\n",
    "        ffn_output = self.pw_ffn(normalized_out1)\n",
    "        \n",
    "        # 6. 第二个残差连接\n",
    "        # 将 FFN 输出与第一个子层的输出相加\n",
    "        out = out1 + ffn_output\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d7837",
   "metadata": {},
   "source": [
    "因果语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebccbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_transformer_lm(\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    d_model: int,\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int,\n",
    "    rope_theta: float,\n",
    "    weights: dict[str, Tensor],\n",
    "    in_indices: Int[Tensor, \" batch_size sequence_length\"],\n",
    ") -> Float[Tensor, \" batch_size sequence_length vocab_size\"]:\n",
    "    \"\"\"\n",
    "    给定Transformer语言模型的权重和输入索引，\n",
    "    返回在输入索引上运行前向传播的输出。\n",
    "\n",
    "    此函数应使用RoPE。\n",
    "\n",
    "    参数:\n",
    "        vocab_size (int): 要预测的输出词汇表中的唯一项目数。\n",
    "        context_length (int): 一次处理的最大token数。\n",
    "        d_model (int): 模型嵌入和子层输出的维度。\n",
    "        num_layers (int): 要使用的Transformer层数。\n",
    "        num_heads (int): 多头注意力中使用的头数。`d_model`必须\n",
    "            能被`num_heads`整除。\n",
    "        d_ff (int): 前馈内层的维度(第3.3节)。\n",
    "        rope_theta (float): RoPE Θ参数。\n",
    "        weights (dict[str, Tensor]):\n",
    "            我们参考实现的状态字典。{num_layers}指的是\n",
    "            `0`到`num_layers - 1`之间的整数(层索引)。\n",
    "            此字典的键包括:\n",
    "            - `token_embeddings.weight`\n",
    "                Token嵌入矩阵。形状为(vocab_size, d_model)。\n",
    "            - `layers.{num_layers}.attn.q_proj.weight`\n",
    "                所有`num_heads`个注意力头的查询投影。\n",
    "                形状为(num_heads * (d_model / num_heads), d_model)。\n",
    "                行按形状为(num_heads, d_k)的矩阵排序，\n",
    "                所以`attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`。\n",
    "            - `layers.{num_layers}.attn.k_proj.weight`\n",
    "                所有`num_heads`个注意力头的键投影。\n",
    "                形状为(num_heads * (d_model / num_heads), d_model)。\n",
    "                行按形状为(num_heads, d_k)的矩阵排序，\n",
    "                所以`attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`。\n",
    "            - `layers.{num_layers}.attn.v_proj.weight`\n",
    "                所有`num_heads`个注意力头的值投影。\n",
    "                形状为(num_heads * (d_model / num_heads), d_model)。\n",
    "                行按形状为(num_heads, d_v)的矩阵排序，\n",
    "                所以`attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`。\n",
    "            - `layers.{num_layers}.attn.output_proj.weight`\n",
    "                多头自注意力输出投影的权重\n",
    "                形状为((d_model / num_heads) * num_heads, d_model)。\n",
    "            - `layers.{num_layers}.ln1.weight`\n",
    "                变换器块中应用的第一个RMSNorm的仿射变换权重。\n",
    "                形状为(d_model,)。\n",
    "            - `layers.{num_layers}.ffn.w1.weight`\n",
    "                FFN中第一个线性变换的权重。\n",
    "                形状为(d_model, d_ff)。\n",
    "            - `layers.{num_layers}.ffn.w2.weight`\n",
    "                FFN中第二个线性变换的权重。\n",
    "                形状为(d_ff, d_model)。\n",
    "            - `layers.{num_layers}.ffn.w3.weight`\n",
    "                FFN中第三个线性变换的权重。\n",
    "                形状为(d_model, d_ff)。\n",
    "            - `layers.{num_layers}.ln2.weight`\n",
    "                变换器块中应用的第二个RMSNorm的仿射变换权重。\n",
    "                形状为(d_model,)。\n",
    "            - `ln_final.weight`\n",
    "                应用于最终变换器块输出的RMSNorm的仿射变换权重。\n",
    "                形状为(d_model, )。\n",
    "            - `lm_head.weight`\n",
    "                语言模型输出嵌入的权重。\n",
    "                形状为(vocab_size, d_model)。\n",
    "        in_indices (Int[Tensor, \"batch_size sequence_length\"]) 运行语言模型的输入索引张量。形状为(batch_size, sequence_length)，其中\n",
    "            `sequence_length`最多为`context_length`。\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor, \"batch_size sequence_length vocab_size\"]: 每个token的预测未归一化\n",
    "        下一词分布的张量。\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35492e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_lm(nn.Module):\n",
    "    def __init__(self, vocab_size:int, context_length:int, num_layers: int, d_model: int, num_heads: int, d_ff: int, rope_theta: float | None = None):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_emb = Embedding(num_embedding=vocab_size, embedding_dim=d_model),\n",
    "            n_block = nn.ModuleList([Transformer_block(d_model=d_model, num_heads=num_heads, d_ff=d_ff, max_seq_len=context_length, theta=rope_theta) for _ in range(num_layers)]),\n",
    "            rmsn_l = RMSNorm(d_model=d_model, eps=1e-5)\n",
    "        ))\n",
    "        self.linear_emb = Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        tkemb = self.transformer.token_emb(x)\n",
    "        for block in self.transformer.n_block:\n",
    "            tkemb = block(tkemb)\n",
    "        tkemb = self.transformer.rmsn_l(tkemb)\n",
    "        out = self.linear_emb(tkemb)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x: torch.Tensor, max_gen_tokens: int, temperature: float = 1.0, top_p: int | None = None, eos_token_id: int | None = None):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        original_sequence_length = x.size(-1)\n",
    "        for _ in range(max_gen_tokens):\n",
    "            x = x[:, -self.context_length :] if x.size(1) > self.context_length else x\n",
    "            logits = self.forward(x)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            temperature_scaled = next_token_logits / temperature\n",
    "            if top_p:\n",
    "                sorted_logits, sorted_indices = torch.sort(temperature_scaled, descending=True)\n",
    "                sorted_probs = run_softmax(sorted_logits, dim=-1)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                sorted_mask = cumulative_probs > top_p\n",
    "                sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "                sorted_mask[..., 0] = False\n",
    "                mask = sorted_mask.scatter(1, sorted_indices, sorted_mask)\n",
    "                temperature_scaled = temperature_scaled.masked_fill(mask, float(\"-inf\"))\n",
    "            probs = run_softmax(temperature_scaled, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            if eos_token_id is not None and next_token_id.item() == eos_token_id:\n",
    "                break\n",
    "            x = torch.cat((x, next_token_id), dim=-1)\n",
    "        return x[:, original_sequence_length:]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_path: str):\n",
    "        with open(os.path.join(pretrained_path, \"model_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        model = cls(**config)\n",
    "        weights_path = os.path.join(pretrained_path, \"model.pt\")\n",
    "        state_dict = torch.load(weights_path, weights_only=True)\n",
    "        # Remove _orig_mod. prefix that comes from serializing a compiled model\n",
    "        unwanted_prefix = \"_orig_mod.\"\n",
    "        for k, _ in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "    def save_pretrained(self, pretrained_path: str):\n",
    "        os.makedirs(pretrained_path, exist_ok=True)\n",
    "        config = {\n",
    "            \"vocab_size\": self.transformer[\"token_emb\"].weight.size(0),\n",
    "            \"context_length\": self.context_length,\n",
    "            \"num_layers\": len(self.transformer[\"n_block\"]),\n",
    "            \"d_model\": self.transformer[\"token_emb\"].weight.size(1),\n",
    "            \"num_heads\": self.transformer[\"n_block\"][0].num_heads,\n",
    "            \"d_ff\": self.transformer[\"n_block\"][0].pw_ffn.d_ff,\n",
    "            \"rope_theta\": self.transformer[\"n_block\"][0].theta\n",
    "        }\n",
    "        with open(Path(pretrained_path) / \"model_config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        torch.save(self.state_dict(), Path(pretrained_path) / \"model.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
