{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7900a131",
   "metadata": {},
   "source": [
    "在特征维度进行归一化，不要剪均值，只用除方差  \n",
    "1.计算特征维度的RMS值，2.除以这个标量 +eps 3.乘个缩放向量\n",
    "\n",
    "在执行归一化操作（如上所述）之前，请先将输入数据类型转换为 torch.float32（之后可转换回原始数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rmsnorm(\n",
    "    d_model: int,\n",
    "    eps: float,\n",
    "    weights: Float[Tensor, \" d_model\"],\n",
    "    in_features: Float[Tensor, \" ... d_model\"],\n",
    ") -> Float[Tensor, \" ... d_model\"]:\n",
    "    \"\"\"\n",
    "    给定RMSNorm仿射变换的权重，\n",
    "    返回在输入特征上运行RMSNorm的输出。\n",
    "\n",
    "    参数:\n",
    "        d_model (int): RMSNorm输入的维度。\n",
    "        eps: (float): 为数值稳定性添加到分母的值。\n",
    "        weights (Float[Tensor, \"d_model\"]): RMSNorm权重。\n",
    "        in_features (Float[Tensor, \"... d_model\"]): 运行RMSNorm的输入特征。可以有任意的前导\n",
    "            维度。\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor,\"... d_model\"]: 与`in_features`形状相同的张量，包含在\n",
    "        `in_features`上运行RMSNorm的输出。\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2a535",
   "metadata": {},
   "source": [
    "有一个可以学习的向量，增强表达性，layer norm中也有添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfde27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    RMS Layer Normalization (Root Mean Square Layer Normalization)\n",
    "    \n",
    "    RMSNorm 是 LayerNorm 的简化版本，只进行缩放操作，不进行平移（centering）。\n",
    "    相比传统 LayerNorm，RMSNorm 计算更高效，在大模型中广泛使用。\n",
    "    \n",
    "    数学公式：\n",
    "    RMS(x) = sqrt(mean(x^2) + eps)\n",
    "    output = (x / RMS(x)) * weight\n",
    "    \n",
    "    其中：\n",
    "    - x: 输入特征 (..., d_model)\n",
    "    - weight: 可学习的缩放参数 (d_model,)\n",
    "    - eps: 数值稳定性参数，防止除零\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "        初始化 RMSNorm 层\n",
    "        \n",
    "        参数:\n",
    "            d_model (int): 输入特征的最后一个维度大小\n",
    "            eps (float): 数值稳定性参数，防止RMS为0时的除零错误\n",
    "                        默认1e-5，通常取值范围 [1e-8, 1e-5]\n",
    "            device: 参数存储的设备 (cpu/cuda)\n",
    "            dtype: 参数的数据类型 (如 torch.float32)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 使用 factory_kwargs 确保所有参数在相同设备和数据类型\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        \n",
    "        # 存储超参数\n",
    "        self.eps = eps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 可学习的缩放参数，初始化为全1\n",
    "        # 形状: (d_model,) - 对应输入的最后一个维度\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, **factory_kwargs))\n",
    "\n",
    "    def forward(self, x: Float[Tensor, \"... d_model\"]) -> Float[Tensor, \"... d_model\"]:\n",
    "        \"\"\"\n",
    "        RMSNorm 前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量，形状为 (..., d_model)\n",
    "               可以是任意维度，但最后一维必须等于 d_model\n",
    "               例如: (batch_size, seq_len, d_model) 或 (batch_size, d_model)\n",
    "        \n",
    "        返回:\n",
    "            规范化后的张量，形状与输入相同 (..., d_model)\n",
    "        \n",
    "        计算流程:\n",
    "        1. 保存原始数据类型\n",
    "        2. 转换到 float32 以提高数值稳定性\n",
    "        3. 计算 RMS (Root Mean Square)\n",
    "        4. 进行规范化和缩放\n",
    "        5. 转换回原始数据类型\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. 保存输入的原始数据类型，用于最后恢复\n",
    "        # 这样做是为了支持混合精度训练 (如 float16)\n",
    "        in_dtype = x.dtype\n",
    "        \n",
    "        # 2. 转换到 float32 进行计算，提高数值稳定性\n",
    "        # 避免在 float16 下的数值误差和溢出问题\n",
    "        x = x.to(torch.float32)\n",
    "        \n",
    "        # 3. 计算 RMS (Root Mean Square)\n",
    "        # x.pow(2): 逐元素平方, 形状不变 (..., d_model)\n",
    "        # .mean(dim=-1, keepdim=True): 沿最后一维求均值\n",
    "        #   - dim=-1: 对最后一维 (d_model) 求均值\n",
    "        #   - keepdim=True: 保持维度，结果形状为 (..., 1)\n",
    "        # + self.eps: 加上小常数防止开根号时除零\n",
    "        # torch.sqrt: 开平方根得到 RMS\n",
    "        RMS_x = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # 4. RMSNorm 变换\n",
    "        # x / RMS_x: 归一化，将每个样本的RMS缩放到1\n",
    "        #   广播机制: (..., d_model) / (..., 1) = (..., d_model)\n",
    "        # * self.weight: 应用可学习的缩放参数\n",
    "        #   广播机制: (..., d_model) * (d_model,) = (..., d_model)\n",
    "        result = x / RMS_x * self.weight\n",
    "        \n",
    "        # 5. 转换回原始数据类型\n",
    "        # 支持混合精度训练，保持与输入相同的精度\n",
    "        return result.to(in_dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
