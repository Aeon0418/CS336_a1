{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637717b7",
   "metadata": {},
   "source": [
    "Attention 的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb18b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "def run_scaled_dot_product_attention(\n",
    "    Q: Float[Tensor, \" ... queries d_k\"],\n",
    "    K: Float[Tensor, \" ... keys d_k\"],\n",
    "    V: Float[Tensor, \" ... values d_v\"],\n",
    "    mask: Float[Tensor, \" ... queries keys\"] | None = None,\n",
    ") -> Float[Tensor, \" ... queries d_v\"]:\n",
    "    \"\"\"\n",
    "    缩放点积注意力 (Scaled Dot-Product Attention) 实现\n",
    "    \n",
    "    这是 Transformer 架构的核心组件，通过查询(Q)、键(K)、值(V)三个矩阵\n",
    "    计算注意力权重，实现序列中不同位置间的信息交互。\n",
    "    \n",
    "    数学公式:\n",
    "    Attention(Q,K,V) = softmax(QK^T / √d_k)V\n",
    "    \n",
    "    计算步骤:\n",
    "    1. 计算 Q 和 K 的点积得分矩阵\n",
    "    2. 缩放 (除以 √d_k)\n",
    "    3. 应用掩码 (如果提供)\n",
    "    4. 应用 softmax 得到注意力权重\n",
    "    5. 用权重对 V 进行加权求和\n",
    "\n",
    "    参数:\n",
    "        Q (Float[Tensor, \" ... queries d_k\"]): 查询张量\n",
    "            - queries: 查询序列长度，通常等于目标序列长度\n",
    "            - d_k: 查询/键的特征维度\n",
    "            - 例如: (batch, num_heads, seq_len_q, d_k)\n",
    "            \n",
    "        K (Float[Tensor, \" ... keys d_k\"]): 键张量  \n",
    "            - keys: 键序列长度，通常等于源序列长度\n",
    "            - d_k: 必须与查询的 d_k 相同，确保点积计算有效\n",
    "            - 例如: (batch, num_heads, seq_len_k, d_k)\n",
    "            \n",
    "        V (Float[Tensor, \" ... values d_v\"]): 值张量\n",
    "            - values: 值序列长度，必须与键序列长度相同\n",
    "            - d_v: 值的特征维度，可以与 d_k 不同\n",
    "            - 例如: (batch, num_heads, seq_len_k, d_v)\n",
    "            \n",
    "        mask (Float[Tensor, \" ... queries keys\"] | None): 掩码张量\n",
    "            - 形状: (..., queries, keys)\n",
    "            - 值为 1.0 表示允许注意力，0.0 表示禁止注意力\n",
    "            - 用于实现因果掩码、填充掩码等\n",
    "            - 例如: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor, \" ... queries d_v\"]: 注意力输出\n",
    "            - 形状与查询的前几维相同，最后一维为 d_v\n",
    "            - 例如: (batch, num_heads, seq_len_q, d_v)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 步骤1: 计算注意力得分矩阵 (Q·K^T)\n",
    "    # torch.einsum(\"... q d, ... k d -> ... q k\", Q, K) 等价于:\n",
    "    # Q @ K.transpose(-2, -1)\n",
    "    # \n",
    "    # 输入形状:\n",
    "    #   Q: (..., queries, d_k)\n",
    "    #   K: (..., keys, d_k)  \n",
    "    # 输出形状:\n",
    "    #   score: (..., queries, keys)\n",
    "    #\n",
    "    # 物理意义: score[i,j] 表示第i个查询对第j个键的\"原始兴趣度\"\n",
    "    score = torch.einsum(\"... q d, ... k d -> ... q k\", Q, K) / math.sqrt(K.size(-1))\n",
    "    \n",
    "    # 等价的矩阵乘法写法 (注释掉的代码):\n",
    "    # score = (Q @ K.transpose(-2, -1)) * (1.0 / math.sqrt(K.size(-1)))\n",
    "    \n",
    "    # 步骤2: 缩放因子 √d_k 的作用\n",
    "    # 除以 √d_k 是为了防止点积值过大，导致 softmax 饱和\n",
    "    # \n",
    "    # 原理: 如果 Q 和 K 的元素是独立同分布的，方差为 σ²\n",
    "    # 那么点积 Q·K 的方差约为 d_k·σ²\n",
    "    # 除以 √d_k 将方差稳定在 σ²，避免梯度消失\n",
    "    \n",
    "    # 步骤3: 应用掩码 (如果提供)\n",
    "    if mask is not None:\n",
    "        # masked_fill: 将 mask==0.0 位置的得分设为负无穷\n",
    "        # 这样在 softmax 后，这些位置的注意力权重会变成 0\n",
    "        #\n",
    "        # 常见掩码类型:\n",
    "        # 1. 因果掩码: 防止未来信息泄露 (下三角矩阵)\n",
    "        # 2. 填充掩码: 忽略填充 token\n",
    "        # 3. 自定义掩码: 特定的注意力模式\n",
    "        score = score.masked_fill(mask == 0.0, float('-inf'))\n",
    "        \n",
    "        # 注释掉的替代写法:\n",
    "        # score = score.masked_fill(mask == False, float('-inf'))\n",
    "    \n",
    "    # 步骤4: 应用 softmax 得到注意力权重\n",
    "    # 沿最后一个维度(keys维度)进行 softmax\n",
    "    # 确保每个查询对所有键的注意力权重和为 1\n",
    "    #\n",
    "    # 形状变化:\n",
    "    #   输入: (..., queries, keys) - 原始得分\n",
    "    #   输出: (..., queries, keys) - 概率分布，每行和为1\n",
    "    #\n",
    "    # 物理意义: score[i,j] 现在表示第i个查询对第j个键的注意力权重\n",
    "    score = run_softmax(score, dim=-1)\n",
    "    \n",
    "    # 步骤5: 计算最终的注意力输出\n",
    "    # score @ V: 用注意力权重对值向量进行加权平均\n",
    "    #\n",
    "    # 输入形状:\n",
    "    #   score: (..., queries, keys) - 注意力权重矩阵\n",
    "    #   V: (..., keys, d_v) - 值矩阵\n",
    "    # 输出形状:\n",
    "    #   att: (..., queries, d_v) - 加权后的特征\n",
    "    #\n",
    "    # 物理意义: \n",
    "    # att[i] = Σ_j score[i,j] * V[j]\n",
    "    # 第i个查询的输出是所有值向量的加权平均，权重由注意力决定\n",
    "    att = score @ V\n",
    "    \n",
    "    return att\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757339f",
   "metadata": {},
   "source": [
    "一般是多头的，rope在做点积前做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57df22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import einx\n",
    "from einops import rearrange\n",
    "\n",
    "class Multihead_self_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头自注意力机制 (Multi-Head Self-Attention)\n",
    "    \n",
    "    这是 Transformer 架构的核心组件，通过多个并行的注意力头来捕捉\n",
    "    不同表示子空间中的依赖关系，增强模型的表达能力。\n",
    "    \n",
    "    核心思想:\n",
    "    1. 将输入通过线性投影得到 Q、K、V\n",
    "    2. 将 Q、K、V 分割成多个头\n",
    "    3. 每个头独立计算注意力\n",
    "    4. 拼接所有头的输出\n",
    "    5. 通过输出投影得到最终结果\n",
    "    \n",
    "    数学公式:\n",
    "    MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n",
    "    其中 head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 d_model: int, \n",
    "                 num_heads: int, \n",
    "                 pos_encode: RotaryPositionalEmbedding | None = None, \n",
    "                 theta: float | None = None):\n",
    "        \"\"\"\n",
    "        初始化多头自注意力模块\n",
    "        \n",
    "        参数:\n",
    "            d_model (int): 模型的特征维度，必须能被 num_heads 整除\n",
    "                          例如: 512, 768, 1024 等\n",
    "            num_heads (int): 注意力头的数量\n",
    "                           例如: 8, 12, 16 等\n",
    "            pos_encode (RotaryPositionalEmbedding | None): RoPE 位置编码器\n",
    "                                                         如果不为 None，将应用旋转位置编码\n",
    "            theta (float | None): RoPE 的基础角度参数\n",
    "                                 如果不为 None，表示启用位置编码\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 确保模型维度能被头数整除，这样每个头的维度是整数\n",
    "        assert d_model % num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n",
    "        \n",
    "        # 保存关键参数\n",
    "        self.d_model = d_model           # 模型总维度\n",
    "        self.num_heads = num_heads       # 注意力头数量\n",
    "        self.d_k = d_model // num_heads  # 每个头的键/查询维度\n",
    "        self.d_v = self.d_k             # 每个头的值维度，通常与 d_k 相等\n",
    "        \n",
    "        # 定义线性投影层\n",
    "        # 注意：这些投影层的输出维度是 num_heads * d_k，而不是 d_k\n",
    "        # 这样可以一次性为所有头生成 Q、K、V，然后再分割\n",
    "        self.q_proj = Linear(self.d_model, self.num_heads * self.d_k)\n",
    "        self.k_proj = Linear(self.d_model, self.num_heads * self.d_k) \n",
    "        self.v_proj = Linear(self.d_model, self.num_heads * self.d_v)\n",
    "        \n",
    "        # 输出投影：将所有头的结果拼接后投影回原始维度\n",
    "        self.o_proj = Linear(self.num_heads * self.d_v, self.d_model)\n",
    "        \n",
    "        # 位置编码相关\n",
    "        self.pos_encode = pos_encode  # RoPE 编码器实例\n",
    "        self.theta = theta           # 位置编码参数，决定是否启用位置编码\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor, \n",
    "                token_positions: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        多头自注意力前向传播\n",
    "        \n",
    "        参数:\n",
    "            x (torch.Tensor): 输入张量，形状 (..., seq_len, d_model)\n",
    "                             通常是 (batch_size, seq_len, d_model)\n",
    "            token_positions (torch.Tensor | None): 位置索引张量\n",
    "                                                  如果为 None，自动生成 [0, 1, 2, ...]\n",
    "                                                  用于 RoPE 位置编码\n",
    "        \n",
    "        返回:\n",
    "            torch.Tensor: 注意力输出，形状与输入相同 (..., seq_len, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. 解析输入张量的形状\n",
    "        # *b 表示除了最后两个维度外的所有批次维度\n",
    "        # 例如：(batch_size, seq_len, d_model) -> b=[batch_size], sequence_length=seq_len\n",
    "        *b, sequence_length, d_model = x.size()\n",
    "        assert d_model == self.d_model, f\"Input d_model ({d_model}) != expected ({self.d_model})\"\n",
    "        \n",
    "        # 2. 通过线性投影生成查询、键、值\n",
    "        # 输入: (..., seq_len, d_model)\n",
    "        # 输出: (..., seq_len, num_heads * d_k)\n",
    "        Q = self.q_proj(x)  # 查询矩阵\n",
    "        K = self.k_proj(x)  # 键矩阵  \n",
    "        V = self.v_proj(x)  # 值矩阵\n",
    "        \n",
    "        # 3. 重塑张量以分离多个头\n",
    "        # 将形状从 (..., seq_len, num_heads * d_k) \n",
    "        # 重塑为 (..., num_heads, seq_len, d_k)\n",
    "        # 这样每个头可以独立处理\n",
    "        Q = rearrange(Q, \"... seq (heads d) -> ... heads seq d\", heads=self.num_heads)\n",
    "        K = rearrange(K, \"... seq (heads d) -> ... heads seq d\", heads=self.num_heads)\n",
    "        V = rearrange(V, \"... seq (heads d) -> ... heads seq d\", heads=self.num_heads)\n",
    "        \n",
    "        # 4. 处理位置编码\n",
    "        # 如果没有提供位置索引，自动生成连续的位置编码\n",
    "        if token_positions is None:\n",
    "            # 生成位置索引 [0, 1, 2, ..., sequence_length-1]\n",
    "            # einx.rearrange 用于为批次维度添加维度\n",
    "            token_positions = einx.rearrange(\n",
    "                \"seq -> b... seq\", \n",
    "                torch.arange(sequence_length, device=x.device), \n",
    "                b=[1] * len(b)  # 为每个批次维度添加大小为1的维度\n",
    "            )\n",
    "        \n",
    "        # 调整位置张量的形状以匹配多头结构\n",
    "        # (..., seq_len) -> (..., 1, seq_len)\n",
    "        # 添加头维度，使其能与 Q、K 广播\n",
    "        token_positions = rearrange(token_positions, \"... seq -> ... 1 seq\")\n",
    "        \n",
    "        # 5. 应用旋转位置编码 (RoPE)\n",
    "        # 只有当提供了 theta 参数时才应用位置编码\n",
    "        if self.theta is not None:\n",
    "            # 对查询和键应用位置编码，值不需要位置信息\n",
    "            Q = self.pos_encode(Q, token_positions)\n",
    "            K = self.pos_encode(K, token_positions)\n",
    "        \n",
    "        # 6. 构建因果掩码 (Causal Mask)\n",
    "        # 创建下三角矩阵，防止模型看到未来的信息\n",
    "        # torch.tril 创建下三角矩阵，上三角部分为0\n",
    "        causal_mask = torch.tril(torch.ones(sequence_length, sequence_length, device=x.device))\n",
    "        \n",
    "        # 调整掩码形状以匹配注意力得分矩阵\n",
    "        # (seq_len, seq_len) -> (1, 1, seq_len, seq_len)\n",
    "        # 添加批次和头维度，便于广播\n",
    "        causal_mask = causal_mask.view(1, 1, sequence_length, sequence_length)\n",
    "        \n",
    "        # 7. 计算缩放点积注意力\n",
    "        # 每个头独立计算注意力，但可以并行处理\n",
    "        # 输入形状: (..., num_heads, seq_len, d_k)\n",
    "        # 输出形状: (..., num_heads, seq_len, d_v)\n",
    "        att = run_scaled_dot_product_attention(Q=Q, K=K, V=V, mask=causal_mask)\n",
    "        \n",
    "        # 8. 拼接多头的输出\n",
    "        # 将多头的结果拼接成一个大的特征向量\n",
    "        # 形状变化: (..., num_heads, seq_len, d_v) -> (..., seq_len, num_heads * d_v)\n",
    "        # .contiguous() 确保内存连续，提高后续计算效率\n",
    "        att = rearrange(att, \"... heads seq d_v -> ... seq (heads d_v)\").contiguous()\n",
    "        \n",
    "        # 9. 输出投影\n",
    "        # 将拼接后的多头特征投影回原始维度\n",
    "        # (..., seq_len, num_heads * d_v) -> (..., seq_len, d_model)\n",
    "        out = self.o_proj(att)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
