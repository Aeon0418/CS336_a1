{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c09d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.model.get_tokenizer import Tokenizer,get_tokenizer\n",
    "\n",
    "#获取Tokenizer类，和get_tokenizer函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769118d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"/home/aeon/Desktop/Learn/CS336/Assignment/assignment1-basics/tokenizer_models/vocab.pkl\"\n",
    "merges_path = \"/home/aeon/Desktop/Learn/CS336/Assignment/assignment1-basics/tokenizer_models/merges.pkl\"\n",
    "special_tokens=[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328fc5f",
   "metadata": {},
   "source": [
    "PKL文件是Python的pickle模块生成的二进制序列化文件，用于保存Python对象。\n",
    "vocab是一个字典，键是int（token——id），值是bytes对象（token）\n",
    "merges是列表，里面都是元组，表示学到的合并规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a909e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer.from_files(vocab_path, merges_path, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54baef6",
   "metadata": {},
   "source": [
    "可以直接接受 词汇表dict[int, bytes] 和 merges表list[tuple[bytes, bytes]]来初始化\n",
    "\n",
    "也可以from_files初始化 # 从pkl文件加载，然后调用__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc6311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1183, 45, 1569, 34, 1268, 431, 259, 2569, 47]\n",
      "Decoded: Hello, world! This is a test.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! This is a test.\"\n",
    "encoded = my_tokenizer.encode(text)\n",
    "print(\"Encoded:\", encoded)\n",
    "decoded = my_tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a5c8c",
   "metadata": {},
   "source": [
    "主要使用 encode 和 decode 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2da62c",
   "metadata": {},
   "source": [
    "## 使用两个pkl，将原文本txt，转化成全是token id 的bin 文件\n",
    "\n",
    "每个token id占用2字节就可以了，使用np.unit16整形来存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc66f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.train_model.train_transformer_nosweep import Tokenizer_TinyStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile_name = 'TinyStoriesV2-GPT4-train.txt'     # 训练数据文件\n",
    "validfile_name = 'TinyStoriesV2-GPT4-valid.txt'     # 验证数据文件\n",
    "trainencode_name = 'TStrain_tokens.bin'             # 编码后的训练数据\n",
    "validencode_name = 'TSvalid_tokens.bin'             # 编码后的验证数据\n",
    "\n",
    "vocab_name = \"vocab.pkl\"\n",
    "merges_name = \"merges.pkl\"\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "\n",
    "# 要花两小时，应该要并行化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e16a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对数据集进行token化...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15600057/15600057 [2:03:48<00:00, 2099.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集token化完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157832/157832 [01:13<00:00, 2137.43it/s]\n"
     ]
    }
   ],
   "source": [
    "Tokenizer_TinyStories(trainfile_name, validfile_name, trainencode_name, validencode_name, vocab_name, merges_name, special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265088b2",
   "metadata": {},
   "source": [
    "这两个文件非常大 1g和10mb\n",
    "trian。bin用来训练，vaild。bin用来算loss。\n",
    "\n",
    "np.memmap 是 NumPy 的内存映射文件功能，它允许你像操作 NumPy 数组一样操作磁盘上的大文件，而不需要将整个文件加载到内存中。  \n",
    "内存映射方式  \n",
    "train_dataset = np.memmap(file_path, dtype=np.uint16, mode='r')  # 几乎不占内存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
