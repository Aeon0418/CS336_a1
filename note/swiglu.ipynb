{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e57c759",
   "metadata": {},
   "source": [
    "激活函数。  \n",
    "relu，与0和自己做max，最简单的非线性函数  \n",
    "主要提供非线性性，早期nn解决不了异或问题，故引入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b6861",
   "metadata": {},
   "source": [
    "silu = x * sigmoid x\n",
    "更光滑，处处可微\n",
    "常称为Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9a8e0",
   "metadata": {},
   "source": [
    "### MLP层的改进\n",
    "\n",
    "数学表示： \n",
    "GLU(x, W1, W2) = σ(W1x) ⊙ W2x, \n",
    " \n",
    "中间是Hadamard product，，σ(W1x)相当于门控，决定哪些权重更重要  \n",
    "比MLP表达力更强，（多一倍的参数）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b6e33",
   "metadata": {},
   "source": [
    "将σ改成swish，就得到了SwiGLU。目前常用的FFN层。  \n",
    "\n",
    "\n",
    "FFN(x) = SwiGLU(x, W, W, W) = W(SiLU(Wx) ⊙ Wx)，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26868639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_swiglu(\n",
    "    d_model: int,\n",
    "    d_ff: int,\n",
    "    w1_weight: Float[Tensor, \" d_ff d_model\"],\n",
    "    w2_weight: Float[Tensor, \" d_model d_ff\"],\n",
    "    w3_weight: Float[Tensor, \" d_ff d_model\"],\n",
    "    in_features: Float[Tensor, \" ... d_model\"],\n",
    ") -> Float[Tensor, \" ... d_model\"]:\n",
    "\n",
    "参数:\n",
    "        d_model (int): 前馈输入和输出的维度。\n",
    "        d_ff (int): SwiGLU内部上投影的维度。\n",
    "        w1_weight (Float[Tensor, \"d_ff d_model\"]): W1的存储权重\n",
    "        w2_weight (Float[Tensor, \"d_model d_ff\"]): W2的存储权重\n",
    "        w3_weight (Float[Tensor, \"d_ff d_model\"]): W3的存储权重\n",
    "        in_features (Float[Tensor, \"... d_model\"]): 前馈层的输入嵌入。\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor, \"... d_model\"]: 与输入嵌入形状相同的输出嵌入。\n",
    "\n",
    "\n",
    "\n",
    "x： ...,d_model\n",
    "\n",
    "W1x = ...,d_ff\n",
    "\n",
    "W3x = ...,d_ff\n",
    "\n",
    "W1x ⊙ W3x = ...,d_ff\n",
    "\n",
    "W2  = ...,d_model,d_ff\n",
    "out = W2(W1x ⊙ W3x).    ... ,d_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用的模块：为了数值稳定性，可以使用 torch.sigmoid()\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # 移除不必要的d_ff调整，直接使用传入的d_ff\n",
    "        self.d_ff = d_ff\n",
    "        self.w1 = Linear(d_model, self.d_ff)\n",
    "        self.w2 = Linear(self.d_ff, d_model)\n",
    "        self.w3 = Linear(d_model, self.d_ff)\n",
    "\n",
    "    def load_weights(self, w1: torch.Tensor, w2: torch.Tensor, w3: torch.Tensor):\n",
    "        \"\"\"加载权重，确保形状匹配\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 直接复制权重，不需要转置（因为Linear类已经处理了）\n",
    "            self.w1.weight.data = w1\n",
    "            self.w2.weight.data = w2\n",
    "            self.w3.weight.data = w3\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SwiGLU前向传播：\n",
    "        SwiGLU(x) = (SiLU(xW1) ⊙ xW3)W2\n",
    "        \"\"\"\n",
    "        w1_out = self.w1(x)        # x @ W1\n",
    "        w3_out = self.w3(x)        # x @ W3\n",
    "        silu_out = w1_out * torch.sigmoid(w1_out)  # SiLU(xW1)\n",
    "        gated = silu_out * w3_out   # SiLU(xW1) ⊙ xW3\n",
    "        return self.w2(gated)       # (SiLU(xW1) ⊙ xW3)W2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
