{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1ef603",
   "metadata": {},
   "source": [
    "linear层，现在都不带bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Int\n",
    "# JAXTyping 是一个为 JAX、PyTorch 等深度学习框架提供的类型注解库，用于标注张量的形状和数据类型。\n",
    "def run_linear(\n",
    "    d_in: int,\n",
    "    d_out: int,\n",
    "    weights: Float[Tensor, \" d_out d_in\"],\n",
    "    in_features: Float[Tensor, \" ... d_in\"],\n",
    ") -> Float[Tensor, \" ... d_out\"]:\n",
    "\n",
    "线性层主要的作用是改变特征维度，必须可微，最简单的就是投影了\n",
    "会输入 d_in 和 d_out 作为输入特征和输出特征的维度，\n",
    "weights 作为权重矩阵，in_features 作为输入特征张量。\n",
    "\n",
    "weights的数据类型是Float（浮点数），张量类型是torch的Tensor，形状为 \"d_out d_in\"，表示输出特征维度和输入特征维度的矩阵。\n",
    "in_features \" ... d_out\" 表示输入特征张量的形状，其中 \"...\" 表示可以有任意数量的前置维度。\n",
    "比如可能是 batch ，seq_len, d_in\n",
    "\n",
    "那么很简单，[seq_len, d_in] 矩阵乘[d_in, d_out] 矩阵\n",
    "就可以了\n",
    "拿in_features的后两维，与weights的转置进行矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25cb87",
   "metadata": {},
   "source": [
    "参数:\n",
    "        d_in (int): 输入维度的大小\n",
    "        d_out (int): 输出维度的大小\n",
    "        weights (Float[Tensor, \"d_out d_in\"]): 要使用的线性权重\n",
    "        in_features (Float[Tensor, \"... d_in\"]): 要应用函数的输出张量\n",
    "\n",
    "    返回:\n",
    "        Float[Tensor, \"... d_out\"]: 线性模块的变换输出。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_features,out_features,device=None,dtype=None):\n",
    "        super().__init__()\n",
    "            self.in_features = in_features\n",
    "            self.out_features = out_features\n",
    "            # 创建权重参数矩阵 W: (out_features, in_features)\n",
    "            # 注意：存储为W而不是W的转置，便于内存访问\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.empty(\n",
    "                    out_features,\n",
    "                    in_features,\n",
    "                    device=device,\n",
    "                    dtype=dtype\n",
    "                )\n",
    "            )\n",
    "            # 初始化权重\n",
    "            self._reset_parameters()\n",
    "\n",
    "        def _reset_parameters(self): \n",
    "            \"\"\"\n",
    "            初始化空的矩阵\n",
    "            \"\"\"\n",
    "            # 使用截断正态分布初始化权重\n",
    "            # std = sqrt(2 / (in_features + out_features)) 是常用的Xavier初始化变体\n",
    "            std = (2.0 / (self.in_features + self.out_features)) ** 0.5\n",
    "            torch.nn.init.trunc_normal_(self.weight, std=std)\n",
    "\n",
    "        def forward(self,x: Tensor)-> Tensor:\n",
    "            # 矩阵乘法有广播机制，只会对最后两维进行矩阵乘法\n",
    "            return x @ self.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c30d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einx\n",
    "# 可以使用einx库来简化矩阵乘法的操作\n",
    "\n",
    "def forward(self, x: Tensor) -> Tensor:\n",
    "        # 使用 einx 进行矩阵乘法\n",
    "        # \"... d_in, d_out d_in -> ... d_out\" 表示：\n",
    "        # 输入: (..., d_in)，权重: (d_out, d_in)\n",
    "        # 输出: (..., d_out)\n",
    "        return einx.dot(\"... d_in, d_out d_in -> ... d_out\", x, self.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f02750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传统方式 重塑操作\n",
    "x = x.view(batch_size, seq_len, num_heads, d_head)\n",
    "\n",
    "# einx 方式\n",
    "x = einx.rearrange(\"batch seq (heads d_head) -> batch seq heads d_head\", \n",
    "                   x, heads=num_heads)\n",
    "\n",
    "\n",
    "# 沿着序列长度求和\n",
    "# 传统方式\n",
    "result = x.sum(dim=1)  # (batch, seq, d_model) -> (batch, d_model)\n",
    "\n",
    "# einx 方式\n",
    "result = einx.sum(\"batch [seq] d_model -> batch d_model\", x) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# einx 平均\n",
    "result = einx.mean(\"batch [seq] d_model -> batch d_model\", x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
